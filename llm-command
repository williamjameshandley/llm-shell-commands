#!/usr/bin/env python3
"""Convert natural language to shell commands using LLM APIs."""

import sys
import os
import json
import urllib.request
import urllib.error

# Provider configurations
PROVIDERS = {
    "openai": {
        "url": "https://api.openai.com/v1/chat/completions",
        "env_key": "OPENAI_API_KEY",
        "model": "gpt-4o-mini",
    },
    "claude": {
        "url": "https://api.anthropic.com/v1/messages",
        "env_key": "ANTHROPIC_API_KEY",
        "model": "claude-3-5-haiku-20241022",
    },
    "groq": {
        "url": "https://api.groq.com/openai/v1/chat/completions",
        "env_key": "GROQ_API_KEY",
        "model": "llama-3.3-70b-versatile",
    },
    "gemini": {
        "url": "https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent",
        "env_key": "GEMINI_API_KEY",
        "model": "gemini-2.5-flash",
    },
}


def get_context() -> str:
    """Gather shell context for better command generation."""
    import subprocess
    import platform

    cwd = os.getcwd()
    shell = os.environ.get("SHELL", "unknown").split("/")[-1]
    user_type = "root" if os.geteuid() == 0 else "user"

    # Detect OS/distro
    os_info = platform.system()
    if os_info == "Linux":
        try:
            with open("/etc/os-release") as f:
                for line in f:
                    if line.startswith("PRETTY_NAME="):
                        os_info = line.split("=", 1)[1].strip().strip('"')
                        break
        except:
            pass
    elif os_info == "Darwin":
        os_info = "macOS"

    # Virtual environment
    venv = os.environ.get("VIRTUAL_ENV", "")
    venv_info = f"Virtual env: {os.path.basename(venv)}" if venv else ""

    # Git context
    git_info = ""
    try:
        branch = subprocess.check_output(
            ["git", "branch", "--show-current"],
            stderr=subprocess.DEVNULL, timeout=0.2
        ).decode().strip()
        if branch:
            git_info = f"Git branch: {branch}"
    except:
        pass

    # File listing with type indicators (using os.scandir for speed)
    files = []
    project_markers = {
        "package.json": "NodeJS", "requirements.txt": "Python",
        "pyproject.toml": "Python", "Cargo.toml": "Rust",
        "go.mod": "Go", "Makefile": "Make", "Dockerfile": "Docker"
    }
    project_types = set()

    try:
        with os.scandir('.') as it:
            entries = sorted(it, key=lambda e: e.name)
            for entry in entries:
                name = entry.name
                if name in project_markers:
                    project_types.add(project_markers[name])
                if entry.is_dir():
                    name += "/"
                elif entry.is_file() and os.access(entry.path, os.X_OK):
                    name += "*"
                if len(files) < 20:
                    files.append(name)
    except:
        pass

    file_list = '\n'.join(files)
    if len(files) == 20:
        file_list += "\n..."

    project_info = f"Project type: {', '.join(project_types)}" if project_types else ""

    # Build context string (skip empty lines)
    lines = [
        f"Directory: {cwd}",
        f"OS: {os_info}",
        f"Shell: {shell} ({user_type})",
        git_info,
        venv_info,
        project_info,
        f"Files:\n{file_list}" if file_list else ""
    ]
    return '\n'.join(line for line in lines if line)


def call_openai_compatible(url: str, api_key: str, model: str, system: str, prompt: str) -> str:
    """Call OpenAI-compatible API (OpenAI, Groq)."""
    data = json.dumps({
        "model": model,
        "messages": [
            {"role": "system", "content": system},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0
    }).encode()

    req = urllib.request.Request(url, data=data, headers={
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    })

    with urllib.request.urlopen(req, timeout=30) as resp:
        result = json.loads(resp.read().decode())
        return result["choices"][0]["message"]["content"].strip()


def call_claude(api_key: str, model: str, system: str, prompt: str) -> str:
    """Call Anthropic Claude API."""
    data = json.dumps({
        "model": model,
        "max_tokens": 1024,  # Required by Claude API
        "system": system,
        "messages": [{"role": "user", "content": prompt}]
    }).encode()

    req = urllib.request.Request(
        "https://api.anthropic.com/v1/messages",
        data=data,
        headers={
            "Content-Type": "application/json",
            "X-Api-Key": api_key,
            "anthropic-version": "2023-06-01"
        }
    )

    with urllib.request.urlopen(req, timeout=30) as resp:
        result = json.loads(resp.read().decode())
        return result["content"][0]["text"].strip()


def call_gemini(api_key: str, model: str, system: str, prompt: str) -> str:
    """Call Google Gemini API."""
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent"

    data = json.dumps({
        "system_instruction": {"parts": [{"text": system}]},
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {"temperature": 0}
    }).encode()

    req = urllib.request.Request(url, data=data, headers={
        "Content-Type": "application/json",
        "x-goog-api-key": api_key
    })

    with urllib.request.urlopen(req, timeout=30) as resp:
        result = json.loads(resp.read().decode())
        return result["candidates"][0]["content"]["parts"][0]["text"].strip()


def get_shell_command(description: str) -> str:
    """Send description to LLM and get back a shell command."""
    provider_name = os.environ.get("LLM_PROVIDER", "openai").lower()

    if provider_name not in PROVIDERS:
        return f"# Error: Unknown provider '{provider_name}'. Use: {', '.join(PROVIDERS.keys())}"

    provider = PROVIDERS[provider_name]
    api_key = os.environ.get(provider["env_key"])

    if not api_key:
        return f"# Error: {provider['env_key']} not set"

    model = os.environ.get("LLM_MODEL", provider["model"])
    context = get_context()

    system = "You are a shell command generator. Output only the command, nothing else."
    prompt = f"""Convert this natural language description to a single shell command.
Return ONLY the command, no explanation, no markdown, no code blocks.

{context}

Description: {description}"""

    try:
        if provider_name in ("openai", "groq"):
            return call_openai_compatible(provider["url"], api_key, model, system, prompt)
        elif provider_name == "claude":
            return call_claude(api_key, model, system, prompt)
        elif provider_name == "gemini":
            return call_gemini(api_key, model, system, prompt)
    except urllib.error.HTTPError as e:
        return f"# Error: {e.code} {e.reason}"
    except Exception as e:
        return f"# Error: {e}"


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("# Usage: llm-command 'description'", file=sys.stderr)
        print(f"# Providers: {', '.join(PROVIDERS.keys())}", file=sys.stderr)
        print("# Set LLM_PROVIDER env var to choose (default: openai)", file=sys.stderr)
        sys.exit(1)

    description = " ".join(sys.argv[1:])
    print(get_shell_command(description))
